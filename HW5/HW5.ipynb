{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNPetCVC4r11MEXvuM5I4yQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pauline-johnson/machine-learning/blob/master/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXIAfvQ0PTt",
        "colab_type": "text"
      },
      "source": [
        "# Homework 5 - Concepts in Machine Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpDO482o0eov",
        "colab_type": "text"
      },
      "source": [
        "## 1. General Concepts\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_Yhhda83StO",
        "colab_type": "text"
      },
      "source": [
        "### Artificial Intelligence, Machine Learning, and Deep Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r21nt-FHXx_r",
        "colab_type": "text"
      },
      "source": [
        "Artificial intelligence is \"a branch of computer dealing with the simulation of intelligent behavior in computers\" (1). Machine learning is a subset of artificial intelligence. Arthur Samuel defined machine learning as a \"field of study that gives computers the ability to learn without being explicitly programmed\" (1). The process of machine learning can be described as the training of a model to make useful predictions, using a data set (4). Deep learning is a subset of machine learning and usually refers to deep artificial nerual networks (2). Neural networks, which are loosely modeled around the brain, cluster and classify unlabeled data according to commonalities in input data (3).  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-F6-_34Xzeg",
        "colab_type": "text"
      },
      "source": [
        "### Supervised, Unsupervised, and Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4AdBoKLX1lN",
        "colab_type": "text"
      },
      "source": [
        "Supervised learning is a type of machine learning where the model is provided labeled training data, with a correct interpretation attached (expected output) (4). Unsupervised learning detects patterns in data, using an unlabeled training data set. Reinforcement learning deals with agents in an artificial environment that attempt to maximize reward/minimize failure. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r6LakqS9W8l",
        "colab_type": "text"
      },
      "source": [
        "## 2. Basic Concepts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4ZLlihWTWY5",
        "colab_type": "text"
      },
      "source": [
        "Linear models use one weight per feature to make predictions. Linear and logistic regression are types of linear models used in machine learning (5).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oad-nZEJTWin",
        "colab_type": "text"
      },
      "source": [
        "### Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZL6A7Zs9dYP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Linear regression attempts to model the relationship between two variables by fitting a linear curve to the observed data. By convention is it written as (6):\n",
        "\n",
        "####$ y' = b + w_1x_1 $\n",
        "\n",
        "where\n",
        "\n",
        "*   $ y'$ is the predicted label, or desired output\n",
        "*   $b$ is the bias\n",
        "*   $w_1$ is the weight of feature 1\n",
        "*   $x_1$ is a feature, or known input\n",
        "\n",
        "You may add on more features with separate weights ($..+ w_2x_2 + ..$)  for more complex models. The trained model predicts the label ($y'$) with given input values ($x_1, x_2, ..$).  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31i04nMoY1d5",
        "colab_type": "text"
      },
      "source": [
        "### Logisitic regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XfJ7QmGTh1p",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Logistic regression chooses a value between 0 and 1, using a sigmoid function, from a linear model's label (7). It can be written as follows (8):\n",
        "\n",
        "####$ y' = \\frac{1}{1+e^{-z} }$\n",
        "\n",
        "where\n",
        "\n",
        "*   $y'$ is the output for that data point\n",
        "*   $z$ is the linear model's label, or $ z = b + w_1x_1+..+w_nx_n$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NB59zd99aczD",
        "colab_type": "text"
      },
      "source": [
        "#### Sigmoid Function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG4INvmmYbSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "outputId": "49fc736a-2bd3-447a-daec-6922aee8d7d4"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "\n",
        "z = np.linspace(-10,10,100)\n",
        "plt.plot(z, sigmoid(z))\n",
        "plt.xlabel('z')\n",
        "plt.ylabel('y\\'')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfL1pBhAaLyI",
        "colab_type": "text"
      },
      "source": [
        "In homework 3, we learned how to implement logistic regression from scratch and using a Keras model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA1nw0spdbg7",
        "colab_type": "text"
      },
      "source": [
        "### Gradient function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTd3akU8TOzU",
        "colab_type": "text"
      },
      "source": [
        "The gradient function is the slope of the tangent line at a point on a function, or in other words, how quickly the function is increasing or decreasing at a certain point. It can be represented by:\n",
        "\n",
        "$\\triangledown f(p) = [ \\frac{\\delta f}{\\delta x_1}(p)...\\frac{\\delta f}{\\delta x_n}(p)]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH1QFseZbg4G",
        "colab_type": "text"
      },
      "source": [
        "### Gradient Descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU1ag7eaTS-C",
        "colab_type": "text"
      },
      "source": [
        "Gradient descent is a method of training your model to minimize loss. It provides a direction to go in so that each new set of hyperparameters takes on a lower loss than the last. You choose a random value for $x_1$, which corresponds to some amount of loss. Then, you keep taking gradient steps where the gradient is decreasing (since a negative gradient points in the direction of the greatest decrease of a function). This brings you closer and closer to the minimum loss. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHX-0UW5TBDg",
        "colab_type": "text"
      },
      "source": [
        "### Mini-batch Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v85w6XLZTScu",
        "colab_type": "text"
      },
      "source": [
        "An important type of gradient descent we learned was mini-batch stochastic gradient descent. A batch is the total number of examples used to calculate the gradient every iteration. Stochastic gradient descent uses a batch size of 1 per iteration. Mini-batch SGD uses a batch size between 10 and 1,000 examples per iteration, which are chose randomly. This is more efficient than taking the entire batch at once, but less \"noisily\" than SGD (19)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtb3kDzrYNGj",
        "colab_type": "text"
      },
      "source": [
        "### Learning Rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAOn0A11YQE_",
        "colab_type": "text"
      },
      "source": [
        "The learning rate is used to train a model with gradient descent. For each gradient step, the gradient and the learning rate are multiplied together to bring you closer to the minimum loss. Choosing a good learning rate for your model is important, otherwise you could get stuck in a local minumum (vs. the absolute minimum), or your model could be very inefficient. A learning rate that is too small will take a very long time and one that is too large will continously overshoot the minimum (21)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPPEIaW3Qxqj",
        "colab_type": "text"
      },
      "source": [
        "## 3. Building a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQrhflRWpmat",
        "colab_type": "text"
      },
      "source": [
        "### Building a simple Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0yYM1KLic2P",
        "colab_type": "text"
      },
      "source": [
        "This semester we focused on building Keras models.\n",
        "Below is an excerpt of my homework 3 code (9). The Sequential model is a linear stack of layers. A layer is a data-processing module that takes in input of at least one tensor and outputs at least one tensor (13). They are the building blocks of of a neural network. Most relatively simple models use the sequential model. The input data in homework 3 had two features, so we will add a Dense layer that accepts two-dimensional input with two rows and output with one row, for the label.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNEFtvE8l4mc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "d2b8da91-1e42-4db4-b267-92b5e82d117b"
      },
      "source": [
        "from tensorflow.python.keras.models import Sequential\n",
        "from tensorflow.python.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_shape=(2,)))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1)                 3         \n",
            "=================================================================\n",
            "Total params: 3\n",
            "Trainable params: 3\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM270HBjpZHl",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional Neural Networks\n",
        "\n",
        "A convolutional nerual network, or CNN, is a neural network where at least one of its layers is a convolutional layer (10).  They are often for image recognition, as discussed in lecture for the classication of MNIST digits. Below is an excerpt of the code that we used in lecture (11). In general, a CNN is made up of a combination of convolutional layers, pooling layers, and dense layers (20). In this case, the convolutional layer is the first layer in which we use the pixels in the images to extract features. The input shape is 28x28 since that is the size of the image. The second dense layer has 10 neurons, each of which is a digit class (0,1,..9) (12). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jL4oRuQMrexr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "outputId": "61e97261-28ee-44f2-848c-e7a7cb513a7f"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# define sequential model\n",
        "network = tf.keras.models.Sequential()\n",
        "\n",
        "# add two layers to model\n",
        "network.add(tf.keras.layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "network.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "network.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blzUiYAQQ00r",
        "colab_type": "text"
      },
      "source": [
        "## 4. Compiling a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "durDfRkQtqq3",
        "colab_type": "text"
      },
      "source": [
        "Once you have defined the architecture of your neural network, you need to configure the learning process and compile your model. Keras requires two parameters to do this, a loss function and an optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5ImrIIIxT2G",
        "colab_type": "text"
      },
      "source": [
        "### Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr8FEXJpxbRR",
        "colab_type": "text"
      },
      "source": [
        "A loss function is the quantity that will be minimize during the training of a model (13). Loss functions evaluate how well the model models your dataset. Some common loss functions are binary cross-entropy (for classification models that output a value between 0 and 1) and mean-squared error. A complete list of loss functions can be found at https://keras.io/losses/.\n",
        "\n",
        "Below is a table of popular loss functions from (14). \n",
        "\n",
        "| Problem type              | Last layer activation  | Loss function              | \n",
        "|:-:                        |:-:                     |:-:                         |\n",
        "| Binary classification     | sigmoid                | binary_crossentropy        |\n",
        "| Multiclass, single-label  | softmax                | categorical_crossentropy   |\n",
        "| Mutlticlass, multi-label  | sigmoid                | binary_crossentropy        |\n",
        "| Regression to real values | none                   | mse                        |\n",
        "| Regression to \\[0,1\\]     | sigmoid                | mse or binary_crossentropy |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncsb42S1yEjZ",
        "colab_type": "text"
      },
      "source": [
        "### Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1TfImUVyJya",
        "colab_type": "text"
      },
      "source": [
        "Optimizers tie together loss function and model parameters by updating the model in response to loss function's output. A complete list of optimizers can be found at https://keras.io/optimizers/."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDzUS8aM7f5u",
        "colab_type": "text"
      },
      "source": [
        "### Metric Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optitexB7mMb",
        "colab_type": "text"
      },
      "source": [
        "An optional parameter is a metric function, which is used to judge the performance of the model (14). A complete list of metrics can be found at https://keras.io/metrics/.\n",
        "\n",
        "Below is an example of compiling a model, taken from my homework 3 (9). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq9KcwSnxWBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='sgd',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chDw1fgVQ7eB",
        "colab_type": "text"
      },
      "source": [
        "## 5. Training a model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucb9Th5k-t3n",
        "colab_type": "text"
      },
      "source": [
        "Training a model involves learning it with training data and testing it with test data. To do this, you want to split up your data into training and test sets. It is important that your training and test data are completely separate. If your test data leaks into your training data, you will not have a true understanding of how well your model works. When training a model, you must also specify the number of epochs (iterations) and the batch size. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA5OQUhzj5F9",
        "colab_type": "text"
      },
      "source": [
        "### Overfitting\n",
        "\n",
        "Intuitively, you might think that the lower the loss during training, the better a model is, but this is not always the case. A model that *overfits* achieves low loss during training, but performs poorly when measured with the test data.  This happens when a model is more complex than it needs to be. To remedy this, you might want to choose a less complex model. From (15), \"The less complex an ML model, the more likely that a good empirical result is not just due to the peculiarities of the sample\". \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qab92LlABBBm",
        "colab_type": "text"
      },
      "source": [
        "#### Underfitting\n",
        "\n",
        "Underfitting happens when a model performs poorly on training data. To avoid this, you might want to provide a wider variety of data or make your model more complex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV88LcEjB1or",
        "colab_type": "text"
      },
      "source": [
        "We can train a model like so:\n",
        "```\n",
        "model.fit(training_data, training_labels, epochs=200, batch_size=500,verbose=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ed50aoQ_Vl",
        "colab_type": "text"
      },
      "source": [
        "## 6. Fine-tuning a pretrained model\n",
        "\n",
        "Fine-tuning is when you take a model that has been pre-trained on a large dataset and continue training it on your own smaller dataset. This is very useful when the data that you are trying to apply the pre-trained model to is similar to the training data of that model. \n",
        "In homework 4, I froze the convolutional base and then added layers on top to fine-tune (17). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhpaAnZ3Pa5K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import Xception\n",
        "from keras import layers\n",
        "from keras import models\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "conv_base = Xception(\n",
        "    weights='imagenet', \n",
        "    include_top=False, \n",
        "    input_shape=(150, 150, 3))\n",
        "\n",
        "# Freeze convolutional base.\n",
        "conv_base.trainable = False\n",
        "\n",
        "# Add layers on top to fine-tune.\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "#model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4N7cYB8tP-dF",
        "colab_type": "text"
      },
      "source": [
        "After doing this, we can further improve our model by fine-tuning the upper layers of the pre-trained model (16). We want to use a smaller learning rate since the pre-trained weights are already very accurate and we do not want to distort the model's reliability (18)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQf0j_pYQ04C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "489ce490-2ef7-48d0-e93a-9ae9f5e50a81"
      },
      "source": [
        "conv_base.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'conv2d_3':\n",
        "    set_trainable = True\n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    loss='binary_crossentropy',\n",
        "    # choose a smaller learning rate\n",
        "    optimizer=optimizers.RMSprop(lr=1e-5), \n",
        "    metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Model)             (None, 5, 5, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 256)               13107456  \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 33,969,193\n",
            "Trainable params: 13,107,713\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqWr9btvXNAM",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAsozp22XSxr",
        "colab_type": "text"
      },
      "source": [
        "As you can see, I learned a lot in this course! I found Google's crash course in machine learning to be very useful throughout the learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHhpLqF51rP5",
        "colab_type": "text"
      },
      "source": [
        "## Sources\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDnfW7TQumX3",
        "colab_type": "text"
      },
      "source": [
        "1. https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_a_slides.pdf\n",
        "\n",
        "2. https://pathmind.com/wiki/ai-vs-machine-learning-vs-deep-learning\n",
        "\n",
        "3. https://pathmind.com/wiki/neural-network\n",
        "\n",
        "4. https://github.com/schneider128k/machine_learning_course/blob/master/slides/1_b_slides.pdf\n",
        "\n",
        "5. https://developers.google.com/machine-learning/glossary#linear_regression\n",
        "\n",
        "6. https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression\n",
        "\n",
        "7. https://developers.google.com/machine-learning/glossary#logistic_regression\n",
        "\n",
        "8. https://developers.google.com/machine-learning/crash-course/logistic-regression/calculating-a-probability\n",
        "\n",
        "9. https://github.com/pauline-johnson/machine-learning/blob/master/HW3/HW3.ipynb\n",
        "\n",
        "10. https://developers.google.com/machine-learning/glossary/#convolutional_layer\n",
        "\n",
        "11. https://github.com/schneider128k/machine_learning_course/blob/master/mnist_digits_classification_dense_layers.ipynb\n",
        "\n",
        "12. https://towardsdatascience.com/image-classification-in-10-minutes-with-mnist-dataset-54c35b77a38d\n",
        "\n",
        "13. https://github.com/schneider128k/machine_learning_course/blob/master/slides/4_slides.pdf\n",
        "\n",
        "14. https://github.com/schneider128k/machine_learning_course/blob/master/keras_basics.md\n",
        "\n",
        "15. https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting\n",
        "\n",
        "16. https://colab.research.google.com/drive/1uVLIUWdT7--b59vM7NaSHkB-qFcu30jU#scrollTo=dI5rmt4UBwXs\n",
        "\n",
        "17. https://github.com/pauline-johnson/machine-learning/blob/master/HW4/HW4_3a.ipynb\n",
        "\n",
        "18. https://flyyufelix.github.io/2016/10/03/fine-tuning-in-keras-part1.html\n",
        "\n",
        "19. https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_f_slides.pdf\n",
        "\n",
        "20. https://developers.google.com/machine-learning/glossary#convolutional_neural_network\n",
        "\n",
        "21. https://github.com/schneider128k/machine_learning_course/blob/master/slides/2_e_slides.pdf"
      ]
    }
  ]
}
